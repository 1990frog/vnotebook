[TOC]

+ analyze分析过程
+ 相关性查询手段
+ Tmdb实例

# 使用analyze查看分词状态
```
put /text/_doc/1
{
  "name":"Eating an apple a day & keeps the docker away"
}
get /text/_doc/1

{
  "_index" : "text",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "name" : "Eating an apple a day & keeps the docker away"
  }
}

```
分词分析
```
get /text/_analyze
{
  "field":"name",
  "text":"Eating an apple a day & keeps the docker away"
}
```
结果如下
```
{
  "tokens" : [
    {
      "token" : "eating",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "an",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "apple",
      "start_offset" : 10,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "a",
      "start_offset" : 16,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "day",
      "start_offset" : 18,
      "end_offset" : 21,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "keeps",
      "start_offset" : 24,
      "end_offset" : 29,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "the",
      "start_offset" : 30,
      "end_offset" : 33,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "docker",
      "start_offset" : 34,
      "end_offset" : 40,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "away",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 8
    }
  ]
}
```
这是默认的分词方式，如果以结构化的模式重新设置索引，并设置`mapping.analyzer=english`
```
put /text
{
  "settings":{
    "number_of_shards":1,
    "number_of_replicas":1
  },
  "mappings":{
    "properties":{
      "name":{"type":"text","analyzer":"english"}
    }
  }
}
```
运行分析
```
get /text/_analyze
{
  "field":"name",
  "text":"Eating an apple a day & keeps the docker away"
}
```
结果如下，其中复数、标点等都被转换，并且去掉了没有太大意义的词汇如`a`，`an`
```
{
  "tokens" : [
    {
      "token" : "eat",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "appl",
      "start_offset" : 10,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "dai",
      "start_offset" : 18,
      "end_offset" : 21,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "keep",
      "start_offset" : 24,
      "end_offset" : 29,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "docker",
      "start_offset" : 34,
      "end_offset" : 40,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "awai",
      "start_offset" : 41,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 8
    }
  ]
}
```

# 中文分词
```
put /text
{
  "settings":{
    "number_of_shards":1,
    "number_of_replicas":1
  },
  "mappings":{
    "properties":{
      "name":{"type":"text","analyzer":"chinese"}
    }
  }
}
get /text/_mapping

get /text/_analyze
{
  "field":"name",
  "text":"ES内置了很多分词器，但内置的分词器对中文的处理不好。下面通过例子来看内置分词器的处理。在web客户端发起如下的一个REST请求，对英文语句进行分词"
}
```





# GET /my-index/_mapping
